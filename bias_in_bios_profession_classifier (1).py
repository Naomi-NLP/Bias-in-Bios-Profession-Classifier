# -*- coding: utf-8 -*-
"""Bias-in-Bios Profession Classifier

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yYijMSqPYBq8qTQtzPoqgd0W-_EqYHDD
"""

!pip install transformers datasets evaluate scikit-learn

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import evaluate
import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix

from datasets import load_dataset

dataset = load_dataset("LabHC/bias_in_bios")
print(dataset)

print(dataset["train"][0])

model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize(batch):
    return tokenizer(batch["hard_text"], padding="max_length", truncation=True)

encoded = dataset.map(tokenize, batched=True)

# Profession mapping
prof_to_name = {
    0:"accountant",1:"architect",2:"attorney",3:"chiropractor",4:"comedian",5:"composer",
    6:"dentist",7:"dietitian",8:"dj",9:"filmmaker",10:"interior_designer",11:"journalist",
    12:"model",13:"nurse",14:"painter",15:"paralegal",16:"pastor",17:"personal_trainer",
    18:"photographer",19:"physician",20:"poet",21:"professor",22:"psychologist",
    23:"rapper",24:"software_engineer",25:"surgeon",26:"teacher",27:"yoga_teacher"
}

label2id = {label: i for i, label in prof_to_name.items()}
id2label = {i: label for label, i in prof_to_name.items()}

def add_labels(batch):
    batch["labels"] = batch["profession"]
    return batch

encoded = encoded.map(add_labels)

model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=len(prof_to_name),
    id2label=id2label,
    label2id=label2id
)

accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    return accuracy.compute(predictions=predictions, references=labels)

# Select subsets
small_train = encoded["train"].select(range(2000))
small_dev   = encoded["dev"].select(range(200))
small_test  = encoded["test"].select(range(1000))

print(len(small_train), len(small_dev), len(small_test))

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=2,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    logging_dir="./logs",
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train,
    eval_dataset=small_dev,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

preds = trainer.predict(small_test)
y_true = preds.label_ids
y_pred = np.argmax(preds.predictions, axis=1)
genders = small_test["gender"]

print(f"Test set size: {len(y_true)}")

preds = trainer.predict(small_test)
y_true = preds.label_ids
y_pred = np.argmax(preds.predictions, axis=1)
genders = small_test["gender"]  # 0 = male, 1 = female

import numpy as np
from sklearn.metrics import accuracy_score

# Indices for male and female
male_idx = [i for i, g in enumerate(genders) if g == 0]
female_idx = [i for i, g in enumerate(genders) if g == 1]

male_acc = accuracy_score(np.array(y_true)[male_idx], np.array(y_pred)[male_idx])
female_acc = accuracy_score(np.array(y_true)[female_idx], np.array(y_pred)[female_idx])

print(f"Overall Accuracy (All): {accuracy_score(y_true, y_pred):.4f}")
print(f"Male Accuracy: {male_acc:.4f}")
print(f"Female Accuracy: {female_acc:.4f}")
print(f"Accuracy Gap: {abs(male_acc - female_acc):.4f}")

import matplotlib.pyplot as plt

# fairness evaluation results
overall_acc = 0.7730
male_acc = 0.7549
female_acc = 0.7963
gap = abs(female_acc - male_acc)

# Data
labels = ["Overall", "Male", "Female"]
values = [overall_acc, male_acc, female_acc]
colors = ["blue", "green", "black"]

plt.figure(figsize=(6, 5))
bars = plt.bar(labels, values, color=colors)

for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f"{yval:.2f}",
             ha='center', va='bottom')

plt.title(f"Fairness Evaluation")
plt.ylabel("Accuracy")
plt.ylim(0, 1)

plt.show()

from sklearn.metrics import confusion_matrix

def error_rates(y_true, y_pred, genders, group):
    idx = [i for i, g in enumerate(genders) if g == group]
    cm = confusion_matrix(np.array(y_true)[idx], np.array(y_pred)[idx])

    FP = cm.sum(axis=0) - np.diag(cm)   # false positives
    FN = cm.sum(axis=1) - np.diag(cm)   # false negatives
    TP = np.diag(cm)
    TN = cm.sum() - (FP + FN + TP)

    FPR = FP.sum() / (FP.sum() + TN.sum())
    FNR = FN.sum() / (FN.sum() + TP.sum())
    return FPR, FNR

male_fpr, male_fnr = error_rates(y_true, y_pred, genders, 0)
female_fpr, female_fnr = error_rates(y_true, y_pred, genders, 1)

print(f"Male FPR: {male_fpr:.4f}, Male FNR: {male_fnr:.4f}")
print(f"Female FPR: {female_fpr:.4f}, Female FNR: {female_fnr:.4f}")

from transformers import AutoTokenizer

# Save model + tokenizer locally
model.save_pretrained("./saved_model_bias_in_bios")
tokenizer.save_pretrained("./saved_model_bias_in_bios")